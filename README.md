# RAG_PIPELINE
RAG pipeline using LlamaParse, LangChain, Chroma, and Llama.cpp to parse PDFs, generate embeddings, retrieve relevant chunks, and answer questions locally using the LLaMA-Sentient-3B GGUF model.
ğŸ¦™ RAG Pipeline using LlamaParse, LangChain & Llama.cpp
This project demonstrates a full Retrieval-Augmented Generation (RAG) pipeline for querying information from a PDF document using open-source tools like LlamaParse, LangChain, and a local LLaMA.cpp model.

ğŸš€ Features
âœ… PDF Parsing using LlamaParse API for accurate Markdown extraction

âœ… Markdown Chunking via LangChain's MarkdownHeaderTextSplitter

âœ… Semantic Embeddings using HuggingFace all-MiniLM-L6-v2 model

âœ… Vector Store creation with Chroma for persistent similarity search

âœ… Local LLM Inference using llama-cpp-python and Llama-Sentient-3.2-3B GGUF model

âœ… Query Answering using semantic retrieval + prompt-injection-based answering

ğŸ”§ APIs and Models Used
LlamaParse API â€“ for parsing PDFs to clean Markdown

all-MiniLM-L6-v2 (HuggingFace) â€“ for generating embeddings

Chroma (LangChain Vector Store) â€“ for similarity search

llama-cpp-python â€“ for local model inference

LLaMA Model Used: prithivMLmods/Llama-Sentient-3.2-3B-Instruct.Q5_K_M.gguf

ğŸ’¡ How it works
Parse PDF â†’ Extracts clean Markdown using LlamaParse

Chunk Document â†’ Splits into hierarchical sections by markdown headings

Embed & Store â†’ Generates vector embeddings and stores them in Chroma

Retrieve â†’ Finds top-k relevant chunks for a given question

Answer â†’ Uses a local LLaMA model to generate an answer from the chunk

ğŸ“¦ Installation
bash
Copy
Edit
pip install huggingface-hub langchain chromadb llama-cpp-python
âš ï¸ Make sure to set your LLAMAPARSE API key in your environment (e.g., via userdata.get() in Colab).

ğŸ“„ Sample Query
Question: "Which detergent was used to clean the classrooms in the village Attayampatti?"
Answer: Retrieved from most relevant markdown chunk and answered using LLaMA-Sentient 3B locally.
